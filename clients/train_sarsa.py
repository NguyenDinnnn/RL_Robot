import numpy as np
import sys, os
from collections import defaultdict
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.robot_env import GridWorldEnv
import random
import pickle

# ---------------------------
# C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng v√† tham s·ªë
# ---------------------------
# S·ª≠ d·ª•ng m√¥i tr∆∞·ªùng c√≥ waypoint ƒë·ªÉ hu·∫•n luy·ªán cho b√†i to√°n ph·ª©c t·∫°p h∆°n
start = (0,0)
goal = (9,7)
waypoints = [(3,2),(6,5)]
obstacles = [(1,1),(2,3),(4,4),(5,1),(7,6)]
# C·∫¨P NH·∫¨T: TƒÉng max_steps ƒë·ªÉ robot c√≥ ƒë·ªß th·ªùi gian kh√°m ph√°
env = GridWorldEnv(width=10, height=8, start=start, goal=goal, obstacles=obstacles, waypoints=waypoints, max_steps=500)

# C·∫¨P NH·∫¨T: ƒê·ªìng b·ªô h√≥a c√°c tham s·ªë th∆∞·ªüng/ph·∫°t v·ªõi file server.py
# ƒêi·ªÅu n√†y R·∫§T QUAN TR·ªåNG ƒë·ªÉ robot h·ªçc ƒë√∫ng h√†nh vi
env.step_penalty = -2.0
env.revisit_penalty = -3.0
env.waypoint_reward = 30.0
env.goal_reward = 100.0
env.goal_before_waypoints_penalty = -10.0


actions = ['up', 'right', 'down', 'left']
gamma = 0.99      # H·ªá s·ªë chi·∫øt kh·∫•u
alpha = 0.1       # T·ª∑ l·ªá h·ªçc
epsilon = 1.0     # T·ª∑ l·ªá kh√°m ph√° ban ƒë·∫ßu
epsilon_decay = 0.9997 # ƒêi·ªÅu ch·ªânh decay rate cho ph√π h·ª£p v·ªõi s·ªë episodes m·ªõi
min_epsilon = 0.05    # Epsilon t·ªëi thi·ªÉu
episodes = 20000  # C·∫¨P NH·∫¨T: TƒÉng s·ªë l∆∞·ª£ng episodes ƒë·ªÉ h·ªçc t·ªët h∆°n

# ---------------------------
# Chu·∫©n b·ªã Q-table v√† ƒë∆∞·ªùng d·∫´n
# ---------------------------
# T·∫°o folder models n·∫øu ch∆∞a c√≥
models_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "models"))
os.makedirs(models_dir, exist_ok=True)
q_path = os.path.join(models_dir, "sarsa_qtable.pkl")

# Load Q-table n·∫øu ƒë√£ t·ªìn t·∫°i, n·∫øu kh√¥ng th√¨ t·∫°o m·ªõi
if os.path.exists(q_path):
    with open(q_path, "rb") as f:
        # S·ª≠ d·ª•ng defaultdict ƒë·ªÉ kh√¥ng b·ªã l·ªói Key-Error
        loaded_q = pickle.load(f)
        Q = defaultdict(lambda: {a: 0.0 for a in actions})
        Q.update(loaded_q)
    print(f"‚úÖ ƒê√£ t·∫£i Q-table c·ªßa SARSA, t·ªïng s·ªë state ƒë√£ bi·∫øt = {len(Q)}")
else:
    Q = defaultdict(lambda: {a: 0.0 for a in actions})
    print("üÜï Kh√¥ng t√¨m th·∫•y Q-table, t·∫°o m·ªõi.")

# ---------------------------
# H√†m v√† V√≤ng l·∫∑p hu·∫•n luy·ªán
# ---------------------------

def encode_visited(wp_list, visited_set):
    """M√£ h√≥a c√°c waypoints ƒë√£ ƒëi qua th√†nh m·ªôt s·ªë nguy√™n."""
    code = 0
    for i, wp in enumerate(wp_list):
        if wp in visited_set:
            code |= (1 << i)
    return code

def choose_action(state, current_epsilon):
    """Ch·ªçn h√†nh ƒë·ªông b·∫±ng ch√≠nh s√°ch epsilon-greedy."""
    if random.random() < current_epsilon:
        return random.choice(actions)
    # N·∫øu c√≥ nhi·ªÅu h√†nh ƒë·ªông c√πng c√≥ gi√° tr·ªã max, ch·ªçn ng·∫´u nhi√™n m·ªôt trong s·ªë ch√∫ng
    max_q = max(Q[state].values())
    best_actions = [a for a, q in Q[state].items() if q == max_q]
    return random.choice(best_actions)

print(f"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán SARSA v·ªõi {episodes} episodes...")
total_rewards = []
for ep in range(episodes):
    env.reset()
    done = False
    episode_reward = 0
    
    # L·∫•y tr·∫°ng th√°i v√† h√†nh ƒë·ªông ban ƒë·∫ßu
    state_xy = env.get_state()
    visited_code = encode_visited(env.waypoints, env.visited_waypoints)
    state = (state_xy[0], state_xy[1], visited_code)
    
    action = choose_action(state, epsilon)
    
    while not done and env.steps < env.max_steps:
        # Th·ª±c hi·ªán h√†nh ƒë·ªông
        action_idx = actions.index(action)
        next_state_xy, reward, done, _ = env.step(action_idx)
        episode_reward += reward
        
        # L·∫•y tr·∫°ng th√°i v√† h√†nh ƒë·ªông ti·∫øp theo
        next_visited_code = encode_visited(env.waypoints, env.visited_waypoints)
        next_state = (next_state_xy[0], next_state_xy[1], next_visited_code)
        
        next_action = choose_action(next_state, epsilon)
        
        # C·∫≠p nh·∫≠t Q-value theo c√¥ng th·ª©c SARSA
        Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])
        
        # Di chuy·ªÉn ƒë·∫øn tr·∫°ng th√°i v√† h√†nh ƒë·ªông ti·∫øp theo
        state = next_state
        action = next_action

    # Gi·∫£m epsilon sau m·ªói episode
    epsilon = max(min_epsilon, epsilon * epsilon_decay)
    total_rewards.append(episode_reward)
    
    # C·∫¨P NH·∫¨T: In ra ti·∫øn tr√¨nh h·ªçc t·∫≠p
    if (ep + 1) % 100 == 0:
        avg_reward = np.mean(total_rewards[-100:])
        print(f"-> Episode {ep + 1}/{episodes} | Epsilon: {epsilon:.4f} | Avg Reward (last 100): {avg_reward:.2f}")

# ---------------------------
# L∆∞u k·∫øt qu·∫£
# ---------------------------
with open(q_path, "wb") as f:
    pickle.dump(dict(Q), f) # Chuy·ªÉn v·ªÅ dict th∆∞·ªùng tr∆∞·ªõc khi l∆∞u
print(f"üéâ Hu·∫•n luy·ªán SARSA ho√†n t·∫•t! ƒê√£ l∆∞u Q-table t·∫°i {q_path}")